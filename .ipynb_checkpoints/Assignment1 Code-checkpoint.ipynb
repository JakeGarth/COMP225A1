{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deal with dataset 1\n",
      "deal with dataset 2\n",
      "deal with dataset 3\n",
      "deal with dataset 4\n",
      "deal with dataset 5\n",
      "deal with dataset 6\n",
      "deal with dataset 7\n",
      "deal with dataset 8\n",
      "deal with dataset 9\n",
      "deal with dataset 10\n",
      "deal with dataset 11\n",
      "deal with dataset 12\n",
      "deal with dataset 13\n",
      "deal with dataset 14\n",
      "deal with dataset 15\n",
      "deal with dataset 16\n",
      "deal with dataset 17\n",
      "deal with dataset 18\n",
      "deal with dataset 19\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Commented by: James Xi Zheng 12/Aug/2019\n",
    "#please create functions to do the following jobs\n",
    "#1. load dataset ->  sample code availalable in the workshops\n",
    "#2. visualize data -> sample code given\n",
    "#3. remove signal noises -> sample code given\n",
    "#4. extract features -> sample code given\n",
    "#5. prepare training set -> sample code given \n",
    "#6. training the given models -> sample code given\n",
    "#7. test the given models -> sample code given\n",
    "#8. print out the evaluation results -> sample code given\n",
    "\n",
    "#as I said in the lecture, the sample code is completed in a un-professional software engineering style\n",
    "#software refactoring is required\n",
    "#please manage the project using SCRUM sprints and manage the source code using Github\n",
    "#document your progress and think critically what are missing from such IoT application and what are missing to move such IoT application from PoC (proof of concept) to solve real-world life\n",
    "#think with which components added, what kind of real-world problems can be solved by it -> this shall be discussed in the conclusion part in the document\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "#this code iterates through all 19 datasets then displays the first 5 rows\n",
    "    for x in range(1,20):\n",
    "        df = pd.read_csv('dataset/dataset_'+str(x)+'.txt', sep=',', header=None)  # use pandas to read csv file\n",
    "        print( 'Dataset ' + str(x) +' contains %d rows.' % len(df)) \n",
    "        print(df.head()) # show first 5 rows of the dataset\n",
    "        \n",
    "\n",
    "'''\n",
    "This code graphs the sensor data of the wrist for all 19 participants for all 13 activities\n",
    "'''\n",
    "def data_visulization():\n",
    "    print(\"These are the graphs of un-filtered data\")\n",
    "    #These are the the activities\n",
    "    activitiesList = ['Sitting', 'Lying', 'Standing', 'Washing Dishes', 'Vacuuming', 'Sweeping', 'Walking', 'Ascending Stairs', 'Destending Stairs', 'Treadmill Running', 'Bicycling (50W)', 'Bicycling (100W)', 'Rope Jumping']\n",
    "    # read each dataset file\n",
    "    for y in range(1,20):\n",
    "        df = pd.read_csv('dataset/dataset_'+str(y)+'.txt', sep=',', header=None)\n",
    "        #then read each activity \n",
    "        for x in range(1, 14):\n",
    "            print('Person '+ str(y) + ', Activity '+activitiesList[x-1])\n",
    "            df_sitting = df[df[24] == x].values\n",
    "            plt.plot(df_sitting[:, 0:3]) #displays accelerometer\n",
    "            plt.show()\n",
    "            plt.plot(df_sitting[:, 3:6]) #displays gyroscope data\n",
    "            plt.show()\n",
    "            \n",
    "    print(\"Finished displaying un-filtered data\")\n",
    "    \n",
    "'''\n",
    "For raw sensor data, it usually contains noise that arises from different sources, such as sensor mis-\n",
    "calibration, sensor errors, errors in sensor placement, or noisy environments. We could apply filter to remove noise of sensor data\n",
    "to smooth data. In this example code, Butterworth low-pass filter is applied. \n",
    "'''\n",
    "     \n",
    "  \n",
    "def noise_removing():\n",
    "    print(\"These are the graphs after noise removal\")\n",
    "    activitiesList = ['Sitting', 'Lying', 'Standing', 'Washing Dishes', 'Vacuuming', 'Sweeping', 'Walking', 'Ascending Stairs', 'Destending Stairs', 'Treadmill Running', 'Bicycling (50W)', 'Bicycling (100W)', 'Rope Jumping']\n",
    "    #read each dataset file\n",
    "    for y in range(1,20):\n",
    "        df = pd.read_csv('dataset/dataset_'+str(y)+'.txt', sep=',', header=None)\n",
    "    # Butterworth low-pass filter. You could try different parameters and other filters. \n",
    "        #read each activity \n",
    "        for x in range(1, 14):\n",
    "            #apply the  butterworth filter\n",
    "            b, a = signal.butter(4, 0.04, 'low', analog=False)\n",
    "            df_sitting = df[df[24] == x].values\n",
    "            #apply the filtering to each column of code being displayed\n",
    "            for i in range(6):\n",
    "                df_sitting[:,i] = signal.lfilter(b, a, df_sitting[:, i])\n",
    "            print('Person '+ str(y) + ', Activity '+activitiesList[x-1])\n",
    "            #display the graphs of the sensor data for each activity\n",
    "            plt.plot(df_sitting[:, 0:3])\n",
    "            plt.show()\n",
    "            plt.plot(df_sitting[:, 3:6])\n",
    "            plt.show()\n",
    "    \n",
    "\n",
    "'''\n",
    "To build a human activity recognition system, we need to extract features from raw data and create feature dataset for training \n",
    "machine learning models.\n",
    "\n",
    "Please create new functions to implement your own feature engineering. The function should output training and testing dataset.\n",
    "'''\n",
    "#Mean, max, and standard deviation were the features used\n",
    "def feature_engineering_example():\n",
    "    training = np.empty(shape=(0,73)) #array of size 73 because 24 measurements * 3 features + 1 for activity\n",
    "    testing = np.empty(shape=(0, 73))\n",
    "    # read in each dataset file\n",
    "    for i in range(19):\n",
    "        df = pd.read_csv('dataset/dataset_' + str(i + 1) + '.txt', sep=',', header=None)\n",
    "        print('deal with dataset ' + str(i + 1))\n",
    "        #apply the filter, same as the above function\n",
    "        for c in range(1, 14):\n",
    "            activity_data = df[df[24] == c].values\n",
    "            b, a = signal.butter(4, 0.04, 'low', analog=False)\n",
    "            for j in range(24):\n",
    "                activity_data[:, j] = signal.lfilter(b, a, activity_data[:, j])\n",
    "            \n",
    "            #seperate the training and testing data in an 80/20 split\n",
    "            datat_len = len(activity_data)\n",
    "            training_len = math.floor(datat_len * 0.8)\n",
    "            training_data = activity_data[:training_len, :]\n",
    "            testing_data = activity_data[training_len:, :]\n",
    "\n",
    "            # segment in to 1000 data points\n",
    "            training_sample_number = training_len // 1000 + 1\n",
    "            testing_sample_number = (datat_len - training_len) // 1000 + 1\n",
    "\n",
    "            #make the sample data\n",
    "            #extract features for all 24 columns of data, for all 19 participants\n",
    "            #specifically, extracting the min, max and standard deviation of each column\n",
    "            for s in range(training_sample_number):\n",
    "                if s < training_sample_number - 1:\n",
    "                    sample_data = training_data[1000*s:1000*(s + 1), :]\n",
    "                else:\n",
    "                    sample_data = training_data[1000*s:, :]\n",
    "                training = trainingtestingData(training, sample_data) #make a training array to later be put in a csv file\n",
    "                \n",
    "            \n",
    "            for s in range(testing_sample_number):\n",
    "                if s < training_sample_number - 1:\n",
    "                    sample_testingData = testing_data[1000*s:1000*(s + 1), :]\n",
    "                else:\n",
    "                    sample_testingData = testing_data[1000*s:, :]\n",
    "\n",
    "                testing = trainingtestingData(testing, sample_testingData) #make a testing array to later be put in a csv file\n",
    "\n",
    "    df_training = pd.DataFrame(training)\n",
    "    df_testing = pd.DataFrame(testing)\n",
    "    #put the data in to CSV files to be read later\n",
    "    df_training.to_csv('training_data.csv', sep=',', header=None)\n",
    "    df_testing.to_csv('testing_data.csv', sep=',', header=None)\n",
    "    \n",
    "                    \n",
    "        \n",
    " #re factored code to not be duplicating       \n",
    "def trainingtestingData(concat, data):\n",
    "    sample = []\n",
    "    for p in range(24):\n",
    "                    sample.append(np.min(data[:, p]))\n",
    "                    sample.append(np.max(data[:, p]))\n",
    "                    sample.append(np.std(data[:, p]))\n",
    "    sample.append(data[0, -1])\n",
    "    sample = np.array([sample])\n",
    "    return np.concatenate((concat, sample), axis=0)\n",
    "\n",
    "'''\n",
    "When we have training and testing feature set, we could build machine learning models to recognize human activities.\n",
    "\n",
    "Please create new functions to fit your features and try other models.\n",
    "'''\n",
    "#This function helped me refactor the code, it essentially makes the y_train and y_test data of\n",
    "def convertData(shape, file):\n",
    "    data = file[shape].values\n",
    "    data = data - 1\n",
    "    data = data.astype(int)\n",
    "    return data\n",
    "\n",
    "def model_training_and_evaluation_KNN():\n",
    "    \n",
    "        df_training = pd.read_csv('training_data.csv', header=None)\n",
    "        df_testing = pd.read_csv('testing_data.csv', header=None)\n",
    "        \n",
    "        #make an array of training values\n",
    "        #refactored the code due to duplication\n",
    "        y_train = convertData(df_training.shape[1] - 1, df_training)\n",
    "        df_training = df_training.drop([df_training.shape[1] - 1], axis=1)\n",
    "        X_train = df_training.values\n",
    "        print (\"training input done \")\n",
    "        #make an array of testing values\n",
    "        #refactored the code due to duplication\n",
    "        y_test = convertData(df_testing.shape[1] - 1,df_testing)\n",
    "        df_testing = df_testing.drop([df_testing.shape[1] - 1], axis=1)\n",
    "        X_test = df_testing.values\n",
    "        print (\"testing input done\")\n",
    "        # Feature normalization for improving the performance of machine learning models. In this example code, \n",
    "            # StandardScaler is used to scale original feature to be centered around zero. You could try other normalization methods.\n",
    "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        print (\"feature normlization done\")\n",
    "\n",
    "         # Build KNN classifier, with 4 neighbours\n",
    "        knn = KNeighborsClassifier(n_neighbors=4)\n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print out the accuracy of the model\n",
    "        print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "            # We could use confusion matrix to view the classification for each activity.\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "def model_training_and_evaluation_SVC():\n",
    "    \n",
    "        df_training = pd.read_csv('training_data.csv', header=None)\n",
    "        df_testing = pd.read_csv('testing_data.csv', header=None)\n",
    "        \n",
    "        #make an array of training value\n",
    "        y_train = convertData(df_training.shape[1] - 1, df_training)\n",
    "        df_training = df_training.drop(df_training.shape[1] - 1, axis=1)\n",
    "        \n",
    "        X_train = df_training.values\n",
    "        print (\"training input done \")\n",
    "        #make an array of testing values\n",
    "        #refractored the code due to duplication\n",
    "        y_test = convertData(df_training.shape[1] - 1, df_training)\n",
    "        df_testing = df_testing.drop(df_training.shape[1] - 1, axis=1)\n",
    "        X_test = df_testing.values\n",
    "        print (\"testing input done\")\n",
    "        # Feature normalization for improving the performance of machine learning models. \n",
    "            # StandardScaler is used to scale original feature to be centered around zero. \n",
    "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        print (\"feature normlization done\")\n",
    "        #these are the parameters for the SVC model\n",
    "        tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-1,1e-2, 1e-3, 1e-4],\n",
    "                         'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 100]},\n",
    "                        {'kernel': ['linear'], 'C': [1e-3, 1e-2, 1e-1, 1, 10, 100]}]\n",
    "        acc_scorer = make_scorer(accuracy_score)\n",
    "        grid_obj  = GridSearchCV(SVC(), tuned_parameters, cv=10, scoring=acc_scorer)\n",
    "        grid_obj  = grid_obj .fit(X_train, y_train)\n",
    "        clf = grid_obj.best_estimator_\n",
    "        print('best clf:', clf)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        #print out the accuracy and confsuion matrix\n",
    "        print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #load_dataset() \n",
    "    #data_visulization()\n",
    "    #noise_removing()\n",
    "    feature_engineering_example()\n",
    "    model_training_and_evaluation_KNN()\n",
    "    #model_training_and_evaluation_SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
